#---Examples
spark-shell

spark.range (1).withColumn("satus", lit("All seems Good. Congrats!")).show(false)
to run in local env - spark-shell --master local[1]

#----Range of numbers
scala> val myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]

#----Actions:
scala> val divisBy2 = myRange.where("number % 2=0")
// divisBy2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: bigint]

scala> divisBy2.count()
//res4: Long = 500

-------------------------------------------------------------------------------------------------------------------------------
## ------HEALTHCARE DATA ANALYSIS PROJECT------------
##---------Using DataBricks-----------------

# File location and type
file_location = "/FileStore/tables/train_2v.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "false"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)
