#---Examples
spark-shell

spark.range (1).withColumn("satus", lit("All seems Good. Congrats!")).show(false)
to run in local env - spark-shell --master local[1]

#----Range of numbers
scala> val myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]

#----Actions:
scala> val divisBy2 = myRange.where("number % 2=0")
// divisBy2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: bigint]

scala> divisBy2.count()
//res4: Long = 500

-------------------------------------------------------------------------------------------------------------------------------
## ------HEALTHCARE DATA ANALYSIS PROJECT------------
##---------Using DataBricks-----------------

# File location and type
file_location = "/FileStore/tables/train_2v.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "false"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)


# -------Create a view or table

temp_table_name = "train_2v_csv"

df.createOrReplaceTempView(temp_table_name)


#------%sql

#-------/* Query the created temp table in a SQL cell */

select * from `train_2v_csv`

permanent_table_name = "train_2v_csv"

##------ Importing pyspark.sql. Setting up Spark

from pyspark.sql import SparkSession
import pyspark.sql as sparksql
spark = SparkSession.builder.appName('stroke').getOrCreate()

#--------Setting path to the file
train = spark.read.csv('/FileStore/tables/train_2v.csv', inferSchema=True,header=True)


#---------CLEANING DATA- renaming "blank rows" under Smoking_status to NO_INFO----

train_f = train.na.fill('NO INFO', subset=['smoking_status'])

#---- importing MEAN package from pyspark
from pyspark.sql.functions import mean

#---- fill in bmi values with mean values
mean = train_f.select(mean(train_f['bmi'])).collect()
mean_bmi = mean[0][0]
#---- A mean of 0 means that the average value in the distribution is 0
train_f = train_f.na.fill(mean_bmi,['bmi'])

display(train_f)

#------- Displaying what age has stroke---
train.groupBy('age','stroke').count().show()

#----View what occupation (work_type )has stroke---
spark.sql("SELECT work_type, count(work_type) as stroke_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY stroke_count").show()


# ----- Displaying rows with strokes = 0 
spark.sql("SELECT * FROM table WHERE stroke = 0 ").show()

#---Display stroke count and percentage on particular occupation---

#----COUNT

spark.sql("SELECT work_type, count(work_type) as stroke_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY stroke_count").show()

#-- Creating a temporary view of table
train.createOrReplaceTempView('table')

#----PERCENTAGE

spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'children') as percentage FROM table WHERE stroke = '1' and work_type = 'children' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Govt_job') as percentage FROM table WHERE stroke = '1' and work_type = 'Govt_job' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Self-employed') as percentage FROM table WHERE stroke = '1' and work_type = 'Self-employed' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Private') as percentage FROM table WHERE stroke = '1' and work_type = 'Private' GROUP BY work_type").show()
