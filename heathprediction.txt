#---Examples
spark-shell

spark.range (1).withColumn("satus", lit("All seems Good. Congrats!")).show(false)
to run in local env - spark-shell --master local[1]

#----Range of numbers
scala> val myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]

#----Actions:
scala> val divisBy2 = myRange.where("number % 2=0")
// divisBy2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: bigint]

scala> divisBy2.count()
//res4: Long = 500

-------------------------------------------------------------------------------------------------------------------------------
## ------HEALTHCARE DATA ANALYSIS PROJECT------------
##---------Using DataBricks-----------------

# File location and type
file_location = "/FileStore/tables/train_2v.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "false"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)


# -------Create a view or table

temp_table_name = "train_2v_csv"

df.createOrReplaceTempView(temp_table_name)


#------%sql

#-------/* Query the created temp table in a SQL cell */

select * from `train_2v_csv`

permanent_table_name = "train_2v_csv"

##------ Importing pyspark.sql. Setting up Spark


from pyspark.sql import SparkSession
import pyspark.sql as sparksql
spark = SparkSession.builder.appName('HeartStroke').getOrCreate()

#--------Setting path to the file
train = spark.read.csv('/FileStore/tables/train_2v.csv', inferSchema=True,header=True)


#---------CLEANING DATA- renaming "blank rows" under Smoking_status to NO_INFO----

train_f = train.na.fill('NO INFO', subset=['smoking_status'])

#---- importing MEAN package from pyspark-------
from pyspark.sql.functions import mean

#---- fill in bmi values with mean values-------
mean = train_f.select(mean(train_f['bmi'])).collect()
mean_bmi = mean[0][0]

#---- A mean of 0 means that the average value in the distribution is 0------
train_f = train_f.na.fill(mean_bmi,['bmi'])

display(train_f)

#----Count of Hypertension, smoking_status, stroke-----
train.groupBy('hypertension').count().show()
train.groupBy('smoking_status').count().show()
train.groupBy('stroke').count().show()


#----Count of married and not married---------
spark.sql("select ever_married, count(ever_married) as count_ever_married from table where stroke == 1 group by ever_married order by count_ever_married").show()

#----View what occupation (work_type )has stroke------
spark.sql("SELECT work_type, count(work_type) as stroke_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY stroke_count").show()

#--total percentage of people in the data----
spark.sql("SELECT work_type, count(work_type) as number_of_people, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table) as percentage FROM table GROUP BY work_type").show()

# ----- Displaying rows with strokes = 0 -----
spark.sql("SELECT * FROM table WHERE stroke = 0 ").show()


#---The data here shows people who never worked has 0 stroke cases, this maybe misleading information in the data ---
spark.sql("SELECT work_type, count(work_type) as number_of_people, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Never_worked') as percentage FROM table WHERE stroke = '0' and work_type = 'Never_worked' GROUP BY work_type").show()


#---Display stroke count and percentage on particular occupation---

#----COUNT---------

spark.sql("SELECT work_type, count(work_type) as stroke_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY stroke_count").show()

#-- Creating a temporary view of table---------
train.createOrReplaceTempView('table')

#----PERCENTAGE----------

spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'children') as percentage FROM table WHERE stroke = '1' and work_type = 'children' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Govt_job') as percentage FROM table WHERE stroke = '1' and work_type = 'Govt_job' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Self-employed') as percentage FROM table WHERE stroke = '1' and work_type = 'Self-employed' GROUP BY work_type").show()
spark.sql("SELECT work_type, count(work_type) as count, (COUNT(work_type) * 100.0) /(SELECT count(work_type) FROM table WHERE work_type == 'Private') as percentage FROM table WHERE stroke = '1' and work_type = 'Private' GROUP BY work_type").show()

#---Counting total strokes and number of strokes recorded above age 40----

train.filter(train['stroke'] == 1).count()
train.filter((train['stroke'] == 1) & (train['age'] > '40')).count()


#---ANALYSIS PHASE----

#---Importing features VectorAssembler,OneHotEncoder, StringIndexer  for performing ML algorithm in catergorical values ----

from pyspark.ml.feature import (VectorAssembler,OneHotEncoder, StringIndexer)

#--- Import Decisiontreeclassifier package---
from pyspark.ml.classification import DecisionTreeClassifier


#----categorizing each column as vectors and indexes-----

gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')
gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')
ever_married_indexer = StringIndexer(inputCol='ever_married', outputCol='ever_marriedIndex')
ever_married_encoder = OneHotEncoder(inputCol='ever_marriedIndex', outputCol='ever_marriedVec')
work_type_indexer = StringIndexer(inputCol='work_type', outputCol='work_typeIndex')
work_type_encoder = OneHotEncoder(inputCol='work_typeIndex', outputCol='work_typeVec')
Residence_type_indexer = StringIndexer(inputCol='Residence_type', outputCol='Residence_typeIndex')
Residence_type_encoder = OneHotEncoder(inputCol='Residence_typeIndex', outputCol='Residence_typeVec')
smoking_status_indexer = StringIndexer(inputCol='smoking_status', outputCol='smoking_statusIndexer')
smoking_status_encoder = OneHotEncoder(inputCol='smoking_statusIndexer', outputCol='smoking_statusVec')


##----Using assembler to use categorical variables in ML algorithm----

assembler = VectorAssembler(inputCols=['genderVec',
'age',
'hypertension',
'heart_disease',
'ever_marriedVec',
'work_typeVec',
'Residence_typeVec',
'avg_glucose_level',
'bmi',
'smoking_statusVec'],outputCol='features')

#---Inserting values to the classifier---
dtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')

#------Import Pipeline-----------
from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[gender_indexer, ever_married_indexer, work_type_indexer, Residence_type_indexer, smoking_status_indexer, gender_encoder, ever_married_encoder, work_type_encoder, Residence_type_encoder,smoking_status_encoder, assembler, dtc])


#-----Perform ML algorithm---
#---Split data to train and test-------------

train_data,test_data = train_f.randomSplit([0.7,0.3], seed = 200)
print("Training Dataset Count: " + str(train.count()))
print("Test Dataset Count: " + str(test_data.count()))


##----- Fitting the train data using pipeline created for train------
model = pipeline.fit(train_data)

#-----transform test data ---
dtc_predictions = model.transform(test_data)

#-------Decision Tree Evaluation -----
#--Import ClassificationEvaluator from pyspark.ml.evaluation-------

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

#----Perform accuracy test using RawPrediction ----

acc_evaluator = MulticlassClassificationEvaluator(labelCol="stroke", predictionCol="prediction", metricName="accuracy")

dtc_acc = acc_evaluator.evaluate(dtc_predictions)
print('A Decision Tree algorithm had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))

#---A Decision Tree algorithm had an accuracy of: 98.38%--- HIGHLY BIASED DATA



